{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "52bb258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e191889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printM3(matrix3, text):\n",
    "    print(f\"\\n{text}:\\n[\")\n",
    "    for i in range(len(matrix3)):\n",
    "        print(matrix3[i])\n",
    "    print(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dbb0476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcFoward(values, weights):\n",
    "    #print(f\"values:{values}\\nweights:{weights}\")\n",
    "    result = 0.0\n",
    "    for i in range(len(weights)-1):\n",
    "        result += values[i] * weights[i]\n",
    "    result += 1 * weights[-1]\n",
    "    #print(f\"\\ncalcFoward:{result}\")\n",
    "    return result\n",
    "\n",
    "def calcCost(value, ideal):\n",
    "    result = (ideal - value)**2/2\n",
    "    #print(f\"Cost:{result}\")\n",
    "    return result\n",
    "\n",
    "def calcdETotalGo(value, ideal):\n",
    "    result = -(ideal - value)\n",
    "    #print(f\"dETotal:{result}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "452b2eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(value):\n",
    "    if(value < 0):\n",
    "        value = 0\n",
    "    print(f\"\\relu:{value}\")\n",
    "    return value\n",
    "\n",
    "def drelu(value):\n",
    "    if(value > 0):\n",
    "        return value\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def linear(value):\n",
    "    return value\n",
    "\n",
    "def dlinear(value):\n",
    "    return 1\n",
    "\n",
    "def sigmoid(value):\n",
    "    try:\n",
    "        value = (1/(1 + np.exp(-value)))\n",
    "    except RuntimeWarning:\n",
    "        value = np.finfo(np.float64).tiny\n",
    "    return value\n",
    "\n",
    "def dsigmoid(value):\n",
    "    value = value * (1 - value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef783a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self,layer_size, neuron_size, activation_function, dactivation_function):\n",
    "        self.neurons = np.random.rand(layer_size, neuron_size+1)\n",
    "        #matrix = np.array(weights)\n",
    "        #self.neurons = matrix\n",
    "        print(f\"\\nNEURONS:{self.neurons}\")\n",
    "        self.activation_function = activation_function\n",
    "        self.dactivation_function = dactivation_function\n",
    "    \n",
    "    def updateWeights(self, update_values, alpha):\n",
    "        #print(f\"Udate Weights\\nUpdate Values:{update_values}\\nAlpha:{alpha}\")\n",
    "        for i in range(len(self.neurons)):\n",
    "            #print(f\"\\nNeuron {i}\")\n",
    "            for j in range(len(self.neurons[i])):\n",
    "                #print(f\"\\nWeight {j}\")\n",
    "                #print(f\"\\nValue:{update_values[i][j]}\")\n",
    "                #print(f\"Equation\\n{self.neurons[i][j]} - {alpha} * {update_values[i][j]}\")\n",
    "                self.neurons[i][j] = self.neurons[i][j] - alpha * update_values[i][j]\n",
    "\n",
    "\n",
    "    def foward(self,values):\n",
    "        layer_a = []\n",
    "        layer_z = []\n",
    "        #print(f\"\\nValues:{values}\")\n",
    "        for i in range(len(self.neurons)):\n",
    "            #print(f\"\\nNeuron:{i}\")\n",
    "            z = calcFoward(values,self.neurons[i])\n",
    "            a = self.activation_function(z)\n",
    "            #print(f\"a:{a}\\nz:{z}\")\n",
    "            layer_a.append(a)\n",
    "            layer_z.append(z)\n",
    "            \n",
    "        #print(f\"\\nLayer A:{layer_a}\\nLayer Z:{layer_z}\")\n",
    "        return np.array(layer_a),np.array(layer_z)\n",
    "    \n",
    "    def backpropagation(self, a0, a1, z0, ideals):\n",
    "        #print(f\"A0:{a0}\\nA1:{a1}\\nZ:{z0}\\nIdeals:{ideals}\")\n",
    "        result = []\n",
    "        aux = []\n",
    "        for i in range(len(self.neurons)):\n",
    "            neuron = []\n",
    "            #print(f\"\\nNeuron:{i}\")\n",
    "            dEg = calcdETotalGo(a0[i],ideals[i])\n",
    "            #print(f\"dEg:{dEg}\")\n",
    "            dgouo = self.dactivation_function(a0[i])\n",
    "            #print(f\"dgouo:{dgouo}\")\n",
    "            #pesos\n",
    "            for j in range(len(self.neurons[i])-1):\n",
    "                #print(f\"\\nWeight:{j}\")\n",
    "                dgh = a1[j]\n",
    "                #print(f\"dgh:{dgh}\")\n",
    "                dEw = dEg * dgouo * dgh\n",
    "                #print(f\"dEw:{dEw}\")\n",
    "                neuron.append(dEw)\n",
    "            #bias\n",
    "            #print(\"\\nBias:\")\n",
    "            dEw = dEg * dgouo\n",
    "            #print(f\"dEw:{dEw}\")\n",
    "            neuron.append(dEw)\n",
    "            #print(f\"Fix Result:{neuron}\")\n",
    "            result.append(np.array(neuron))\n",
    "            aux.append(dEw)\n",
    "            #print(f\"aux {i}:\\n{aux}\")\n",
    "        return np.array(result), np.array(aux)\n",
    "\n",
    "    def backpropagationHidden(self, dEu, a0, a1, previous_layer):\n",
    "        #print(f\"\\nA0:{a0}\\nA1:{a1}\\ndEu:{dEu}\\nWeights:{self.neurons}\\nPrevious Layer:{previous_layer}\")\n",
    "        result = []\n",
    "        next = []\n",
    "        for i in range(len(self.neurons)):\n",
    "            neuron = []\n",
    "            #print(f\"\\nNeuron:{i}\")\n",
    "            dghuh = self.dactivation_function(a0[i])\n",
    "            #print(f\"dghuh:{dghuh}\")\n",
    "            true_dEg = []\n",
    "            for j in range(len(previous_layer)):\n",
    "                for k in range(len(dEu)):\n",
    "                    #print(f\"\\nCalc dEg:{j}\")\n",
    "                    dEg = previous_layer[j][i] * dEu[k] \n",
    "                    #print(f\"W:{previous_layer[j][i]}\\ndEu:{dEu[j]}\\ndEg:{dEg}\")\n",
    "                    true_dEg.append(dEg)\n",
    "            true_dEg = sum(true_dEg)\n",
    "            next.append(true_dEg)\n",
    "            #print(f\"\\nTrue dEg:{true_dEg}\")\n",
    "            #pesos\n",
    "            for k in range(len(self.neurons[i])-1):\n",
    "                #print(f\"\\nWeight:{k}\")\n",
    "                dgh = a1[k]\n",
    "                #print(f\"dgh:{dgh}\")\n",
    "                dEw = true_dEg * dghuh * dgh\n",
    "                #print(f\"\\ndEw:{dEw}\")\n",
    "                neuron.append(dEw)\n",
    "            #bias\n",
    "            #print(f\"\\nBias:\")\n",
    "            dEw = true_dEg * dghuh\n",
    "            #print(f\"\\ndEw:{dEw}\")\n",
    "            neuron.append(dEw)\n",
    "            result.append(np.array(neuron))\n",
    "\n",
    "\n",
    "        return np.array(result),np.array(next)\n",
    "\n",
    "\n",
    "    def relat(self):\n",
    "        print(self.neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9dd6f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def appendLayer(self, layer_size, neuron_size, activation_function, dactivation_function):\n",
    "        self.layers.append(Layer(layer_size, neuron_size, activation_function, dactivation_function))\n",
    "\n",
    "    def foward(self,values):\n",
    "        #print(f\"\\nFirst_Values:{values}\")\n",
    "        layer_a = [values]\n",
    "        layer_z = []\n",
    "        for i in range(len(self.layers)):\n",
    "            #print(f\"\\nFoward:{i}\")\n",
    "            values,z = self.layers[i].foward(values)\n",
    "            layer_a.append(values)\n",
    "            layer_z.append(z)\n",
    "        return layer_a,layer_z\n",
    "                \n",
    "    def backpropagation(self, all_a, all_z, ideals):\n",
    "        #print(f\"\\nBack First\")\n",
    "        #printM3(all_a,\"All A\")\n",
    "        #printM3(all_z,\"All Z\")\n",
    "        #print(f\"\\nIdeals:{ideals}\")\n",
    "        result = []\n",
    "        next = []\n",
    "        for i in range(len(self.layers)):\n",
    "            if i == 0:\n",
    "                #print(f\"\\nBack\")\n",
    "                aux, next = self.layers[-(i+1)].backpropagation(all_a[-(i+1)], all_a[-(i+2)], all_z[-(i+1)], ideals)\n",
    "                result.append(aux)\n",
    "                #print(next)\n",
    "            else:\n",
    "                #print(f\"\\nBack Hidden {i}\")\n",
    "                aux, next = self.layers[-(i+1)].backpropagationHidden(next, all_a[-(i+1)], all_a[-(i+2)], self.layers[-i].neurons)\n",
    "                result.append(aux)\n",
    "        return result\n",
    "        \n",
    "    def calcError(self, output, ideals):\n",
    "        result = 0\n",
    "        #print(f\"Ideals:{ideals}\\nOut:{output}\")\n",
    "        for i in range(len(output)):\n",
    "            result = result + calcCost(output[i], ideals[i])\n",
    "        #print(f\"Total cost:{result}\")\n",
    "        return result\n",
    "    \n",
    "    def updateWeights(self, update_values, alpha):\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[-(i+1)].updateWeights(update_values[i],alpha)\n",
    "\n",
    "    def train(self, input, ideals, alpha, epoch):\n",
    "        print(\"Training\")\n",
    "        acumulated = 0\n",
    "        for i in range(epoch):\n",
    "            error = []\n",
    "            for j in range(len(input)):\n",
    "                all_a, all_z = self.foward(input[j])\n",
    "                #print(f\"\\nAll Activations:\\n{all_a}\")\n",
    "                if(i <= 2 or i >= 998):\n",
    "                    error.append(self.calcError(all_a[-1],ideals[j]))\n",
    "                fix = self.backpropagation(all_a, all_z, ideals[j])\n",
    "                #print(f\"fix:{fix}\")\n",
    "                if type(acumulated) is int:\n",
    "                    acumulated = fix\n",
    "                else:\n",
    "                    for k in range(len(acumulated)):\n",
    "                        for l in range(len(acumulated[k])):\n",
    "                            acumulated[k][l] = acumulated[k][l] + fix[k][l]\n",
    "                #print(f\"\\nacc {j}:\\n{acumulated}\")\n",
    "            if(i <= 2 or i >= epoch-2):\n",
    "                error = sum(error)\n",
    "                error = error / len(input)\n",
    "                print(f\"Total Error Epoch {i}: {error}\")\n",
    "            #print(f\"Acumulated sum:{acumulated}\")\n",
    "            for k in range(len(acumulated)):\n",
    "                for l in range(len(acumulated[k])):\n",
    "                    acumulated[k][l] = acumulated[k][l] / len(input)\n",
    "            #print(f\"Acumulated div:{acumulated}\")\n",
    "            self.updateWeights(acumulated,alpha)\n",
    "\n",
    "    def test(self, input, ideals):\n",
    "        print(\"Testing\")\n",
    "        error = []\n",
    "        for j in range(len(input)):\n",
    "            all_a, all_z = self.foward(input[j])\n",
    "            error.append(self.calcError(all_a[-1],ideals[j]))\n",
    "        error = sum(error)\n",
    "        error = error / len(input)\n",
    "        print(f\"Total Error:{error}\")\n",
    "    \n",
    "    def relat(self):\n",
    "        for i in range(len(self.layers)):\n",
    "            print(f\"\\nLayer{i}:\")\n",
    "            self.layers[i].relat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76244f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NEURONS:[[0.29751782 0.19450498 0.55320811 0.989759   0.94000046]\n",
      " [0.04786209 0.25989237 0.06132024 0.40202024 0.91497649]\n",
      " [0.20806137 0.42566086 0.28683948 0.60648868 0.15091016]\n",
      " [0.71925036 0.6624696  0.85477155 0.42197915 0.22104436]\n",
      " [0.52496676 0.17207643 0.94335375 0.24885774 0.38891095]\n",
      " [0.46721426 0.92492928 0.14848644 0.91786748 0.43208742]\n",
      " [0.32407054 0.09935384 0.30017519 0.06421489 0.66969776]\n",
      " [0.57804906 0.580806   0.05798437 0.42453903 0.76306453]]\n",
      "\n",
      "NEURONS:[[0.46382833 0.54238368 0.22416321 0.72142642 0.91993547 0.13755571\n",
      "  0.81503319 0.30157633 0.44029021]\n",
      " [0.99583545 0.47936873 0.77774964 0.52099784 0.19832643 0.22182506\n",
      "  0.86878377 0.09924862 0.85347978]\n",
      " [0.30203188 0.32601103 0.26522486 0.87808554 0.59811301 0.8040257\n",
      "  0.89452186 0.03318196 0.94807431]\n",
      " [0.56315852 0.91696105 0.952711   0.22153005 0.08965613 0.18677093\n",
      "  0.1742667  0.08239901 0.92405446]]\n",
      "\n",
      "NEURONS:[[0.56166194 0.16293548 0.46136873 0.07486988 0.79460293]]\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "network.appendLayer(8,4,sigmoid,dsigmoid)\n",
    "network.appendLayer(4,8,sigmoid,dsigmoid)\n",
    "network.appendLayer(1,4,sigmoid,dsigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b2766baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer0:\n",
      "[[0.29751782 0.19450498 0.55320811 0.989759   0.94000046]\n",
      " [0.04786209 0.25989237 0.06132024 0.40202024 0.91497649]\n",
      " [0.20806137 0.42566086 0.28683948 0.60648868 0.15091016]\n",
      " [0.71925036 0.6624696  0.85477155 0.42197915 0.22104436]\n",
      " [0.52496676 0.17207643 0.94335375 0.24885774 0.38891095]\n",
      " [0.46721426 0.92492928 0.14848644 0.91786748 0.43208742]\n",
      " [0.32407054 0.09935384 0.30017519 0.06421489 0.66969776]\n",
      " [0.57804906 0.580806   0.05798437 0.42453903 0.76306453]]\n",
      "\n",
      "Layer1:\n",
      "[[0.46382833 0.54238368 0.22416321 0.72142642 0.91993547 0.13755571\n",
      "  0.81503319 0.30157633 0.44029021]\n",
      " [0.99583545 0.47936873 0.77774964 0.52099784 0.19832643 0.22182506\n",
      "  0.86878377 0.09924862 0.85347978]\n",
      " [0.30203188 0.32601103 0.26522486 0.87808554 0.59811301 0.8040257\n",
      "  0.89452186 0.03318196 0.94807431]\n",
      " [0.56315852 0.91696105 0.952711   0.22153005 0.08965613 0.18677093\n",
      "  0.1742667  0.08239901 0.92405446]]\n",
      "\n",
      "Layer2:\n",
      "[[0.56166194 0.16293548 0.46136873 0.07486988 0.79460293]]\n",
      "Training\n",
      "Total Error Epoch 0: 0.3398768658578165\n",
      "Total Error Epoch 1: 0.3395499986090682\n",
      "Total Error Epoch 2: 0.3392502509008307\n",
      "Total Error Epoch 9998: 0.16687323389863543\n",
      "Total Error Epoch 9999: 0.1668732109814873\n",
      "\n",
      "Layer0:\n",
      "[[-1.11608682 -1.91666504  2.82117396  2.16586778  0.40619048]\n",
      " [-1.12831199 -1.92057207  2.94969228  1.86382002  0.38127953]\n",
      " [-1.07364421 -1.74775773  2.9013254   1.9412665  -0.3744842 ]\n",
      " [ 0.72445242  0.66578054  0.85648881  0.42230049  0.22212689]\n",
      " [-0.97999181 -1.8860823   2.97935334  1.37107556 -0.14014673]\n",
      " [ 0.67296057  1.06385497  0.20349411  0.92456805  0.47511176]\n",
      " [-0.99722037 -2.1287626   3.11205991  1.53328343  0.11425584]\n",
      " [ 0.89728355  0.80536521  0.12505021  0.42652614  0.83106281]]\n",
      "\n",
      "Layer1:\n",
      "[[ 1.36977966  1.45111878  1.13200596 -0.59707946  1.77346548 -1.17777771\n",
      "   1.72957315 -1.01515586 -0.88097573]\n",
      " [ 1.48092384  0.9656968   1.26374229 -0.60185722  0.65216775 -0.89961899\n",
      "   1.35784542 -1.02312266 -0.27134349]\n",
      " [ 0.28761069  0.31301908  0.25211942  1.0765525   0.56745383  1.0027078\n",
      "   0.88223422  0.23210438  1.14682551]\n",
      " [ 1.27283863  1.62821666  1.66348051 -0.8947023   0.76338877 -0.92760342\n",
      "   0.88907507 -1.0331455  -0.19444908]]\n",
      "\n",
      "Layer2:\n",
      "[[ 5.56945941  3.24449375 -1.93006725  3.88746507 -2.38089349]]\n"
     ]
    }
   ],
   "source": [
    "def prepare_Y(y):\n",
    "    mapping = {\n",
    "        \"Iris-setosa\": 0,\n",
    "        \"Iris-versicolor\": 1,\n",
    "        \"Iris-virginica\": 2,\n",
    "    }\n",
    "\n",
    "    return y.map(mapping)\n",
    "\n",
    "# Definição do dataframe iris Flowers\n",
    "df = pd.read_csv('IRIS.csv')\n",
    "\n",
    "\n",
    "X = df.drop(columns=['species']).to_numpy()\n",
    "\n",
    "Y = prepare_Y(df['species']).to_numpy()\n",
    "\n",
    "clean_input = []\n",
    "for i in range(len(X)):\n",
    "    clean_input.append(np.array(X[i]))\n",
    "\n",
    "Y = Y.tolist()\n",
    "\n",
    "clean_all_ideals = []\n",
    "for i in range(len(Y)):\n",
    "    aux = [Y[i]]\n",
    "    clean_all_ideals.append(aux)\n",
    "\n",
    "#print(clean_all_ideals)\n",
    "\n",
    "network.relat()\n",
    "\n",
    "network.train(clean_input, clean_all_ideals, 0.5, 10000)\n",
    "\n",
    "network.relat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53db805",
   "metadata": {},
   "source": [
    "#esse era o principal teste, onde Network2 permitia criar a rede com o peso dos neuronios já definidos\n",
    "network2 = Network2()\n",
    "network2.appendLayer(2,2,sigmoid,dsigmoid,[[0.15, 0.2, 0.35], [0.25, 0.3, 0.35]])\n",
    "network2.appendLayer(2,2,sigmoid,dsigmoid,[[0.4, 0.45, 0.6], [0.5, 0.55, 0.6]])\n",
    "\n",
    "input = [[1,1],[0.05,0.1],[1,0],[0,0]]\n",
    "all_ideals = [[0,1],[0.01,0.99],[1,0],[0,1]]\n",
    "\n",
    "clean_input = []\n",
    "for i in range(len(input)):\n",
    "    clean_input.append(np.array(input[i]))\n",
    "\n",
    "clean_all_ideals = np.array(all_ideals)\n",
    "\n",
    "network2.train([clean_input[1]], [clean_all_ideals[1]], 0.5, 10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
