{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "52bb258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3e191889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printM3(matrix3, text):\n",
    "    print(f\"\\n{text}:\\n[\")\n",
    "    for i in range(len(matrix3)):\n",
    "        print(matrix3[i])\n",
    "    print(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dbb0476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcFoward(values, weights):\n",
    "    #print(f\"values:{values}\\nweights:{weights}\")\n",
    "    result = 0.0\n",
    "    for i in range(len(weights)-1):\n",
    "        result += values[i] * weights[i]\n",
    "    result += 1 * weights[-1]\n",
    "    #print(f\"\\ncalcFoward:{result}\")\n",
    "    return result\n",
    "\n",
    "def calcCost(value, ideal):\n",
    "    result = (ideal - value)**2/2\n",
    "    #print(f\"Cost:{result}\")\n",
    "    return result\n",
    "\n",
    "def calcdETotalGo(value, ideal):\n",
    "    result = -(ideal - value)\n",
    "    #print(f\"dETotal:{result}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "452b2eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(value):\n",
    "    if(value < 0):\n",
    "        value = 0\n",
    "    print(f\"\\relu:{value}\")\n",
    "    return value\n",
    "\n",
    "def drelu(value):\n",
    "    if(value > 0):\n",
    "        return value\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def linear(value):\n",
    "    return value\n",
    "\n",
    "def dlinear(value):\n",
    "    return 1\n",
    "\n",
    "def sigmoid(value):\n",
    "    try:\n",
    "        value = (1/(1 + np.exp(-value)))\n",
    "    except RuntimeWarning:\n",
    "        value = np.finfo(np.float64).tiny\n",
    "    return value\n",
    "\n",
    "def dsigmoid(value):\n",
    "    value = value * (1 - value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ef783a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self,layer_size, neuron_size, activation_function, dactivation_function):\n",
    "        self.neurons = np.random.rand(layer_size, neuron_size+1)\n",
    "        #matrix = np.array(weights)\n",
    "        #self.neurons = matrix\n",
    "        print(f\"\\nNEURONS:{self.neurons}\")\n",
    "        self.activation_function = activation_function\n",
    "        self.dactivation_function = dactivation_function\n",
    "    \n",
    "    def updateWeights(self, update_values, alpha):\n",
    "        #print(f\"Udate Weights\\nUpdate Values:{update_values}\\nAlpha:{alpha}\")\n",
    "        for i in range(len(self.neurons)):\n",
    "            #print(f\"\\nNeuron {i}\")\n",
    "            for j in range(len(self.neurons[i])):\n",
    "                #print(f\"\\nWeight {j}\")\n",
    "                #print(f\"\\nValue:{update_values[i][j]}\")\n",
    "                #print(f\"Equation\\n{self.neurons[i][j]} - {alpha} * {update_values[i][j]}\")\n",
    "                self.neurons[i][j] = self.neurons[i][j] - alpha * update_values[i][j]\n",
    "\n",
    "\n",
    "    def foward(self,values):\n",
    "        layer_a = []\n",
    "        layer_z = []\n",
    "        #print(f\"\\nValues:{values}\")\n",
    "        for i in range(len(self.neurons)):\n",
    "            #print(f\"\\nNeuron:{i}\")\n",
    "            z = calcFoward(values,self.neurons[i])\n",
    "            a = self.activation_function(z)\n",
    "            #print(f\"a:{a}\\nz:{z}\")\n",
    "            layer_a.append(a)\n",
    "            layer_z.append(z)\n",
    "            \n",
    "        #print(f\"\\nLayer A:{layer_a}\\nLayer Z:{layer_z}\")\n",
    "        return np.array(layer_a),np.array(layer_z)\n",
    "    \n",
    "    def backpropagation(self, a0, a1, z0, ideals):\n",
    "        #print(f\"A0:{a0}\\nA1:{a1}\\nZ:{z0}\\nIdeals:{ideals}\")\n",
    "        result = []\n",
    "        aux = []\n",
    "        for i in range(len(self.neurons)):\n",
    "            neuron = []\n",
    "            #print(f\"\\nNeuron:{i}\")\n",
    "            dEg = calcdETotalGo(a0[i],ideals[i])\n",
    "            #print(f\"dEg:{dEg}\")\n",
    "            dgouo = self.dactivation_function(a0[i])\n",
    "            #print(f\"dgouo:{dgouo}\")\n",
    "            #pesos\n",
    "            for j in range(len(self.neurons[i])-1):\n",
    "                #print(f\"\\nWeight:{j}\")\n",
    "                dgh = a1[j]\n",
    "                #print(f\"dgh:{dgh}\")\n",
    "                dEw = dEg * dgouo * dgh\n",
    "                #print(f\"dEw:{dEw}\")\n",
    "                neuron.append(dEw)\n",
    "            #bias\n",
    "            #print(\"\\nBias:\")\n",
    "            dEw = dEg * dgouo\n",
    "            #print(f\"dEw:{dEw}\")\n",
    "            neuron.append(dEw)\n",
    "            #print(f\"Fix Result:{neuron}\")\n",
    "            result.append(np.array(neuron))\n",
    "            aux.append(dEw)\n",
    "            #print(f\"aux {i}:\\n{aux}\")\n",
    "        return np.array(result), np.array(aux)\n",
    "\n",
    "    def backpropagationHidden(self, dEu, a0, a1, previous_layer):\n",
    "        #print(f\"\\nA0:{a0}\\nA1:{a1}\\ndEu:{dEu}\\nWeights:{self.neurons}\\nPrevious Layer:{previous_layer}\")\n",
    "        result = []\n",
    "        for i in range(len(self.neurons)):\n",
    "            neuron = []\n",
    "            #print(f\"\\nNeuron:{i}\")\n",
    "            dghuh = self.dactivation_function(a0[i])\n",
    "            #print(f\"dghuh:{dghuh}\")\n",
    "            true_dEg = []\n",
    "            for j in range(len(previous_layer)):\n",
    "                for k in range(len(dEu)):\n",
    "                    #print(f\"\\nCalc dEg:{j}\")\n",
    "                    dEg = previous_layer[j][i] * dEu[k] \n",
    "                    #print(f\"W:{previous_layer[j][i]}\\ndEu:{dEu[j]}\\ndEg:{dEg}\")\n",
    "                    true_dEg.append(dEg)\n",
    "            true_dEg = sum(true_dEg)\n",
    "            #print(f\"\\nTrue dEg:{true_dEg}\")\n",
    "            #pesos\n",
    "            for k in range(len(self.neurons[i])-1):\n",
    "                #print(f\"\\nWeight:{k}\")\n",
    "                dgh = a1[k]\n",
    "                #print(f\"dgh:{dgh}\")\n",
    "                dEw = true_dEg * dghuh * dgh\n",
    "                #print(f\"\\ndEw:{dEw}\")\n",
    "                neuron.append(dEw)\n",
    "            #bias\n",
    "            #print(f\"\\nBias:\")\n",
    "            dEw = true_dEg * dghuh\n",
    "            #print(f\"\\ndEw:{dEw}\")\n",
    "            neuron.append(dEw)\n",
    "            result.append(np.array(neuron))\n",
    "\n",
    "\n",
    "        return np.array(result)\n",
    "\n",
    "\n",
    "    def relat(self):\n",
    "        print(self.neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e9dd6f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def appendLayer(self, layer_size, neuron_size, activation_function, dactivation_function):\n",
    "        self.layers.append(Layer(layer_size, neuron_size, activation_function, dactivation_function))\n",
    "\n",
    "    def foward(self,values):\n",
    "        #print(f\"\\nFirst_Values:{values}\")\n",
    "        layer_a = [values]\n",
    "        layer_z = []\n",
    "        for i in range(len(self.layers)):\n",
    "            #print(f\"\\nFoward:{i}\")\n",
    "            values,z = self.layers[i].foward(values)\n",
    "            layer_a.append(values)\n",
    "            layer_z.append(z)\n",
    "        return layer_a,layer_z\n",
    "                \n",
    "    def backpropagation(self, all_a, all_z, ideals):\n",
    "        #print(f\"\\nBack First\")\n",
    "        #printM3(all_a,\"All A\")\n",
    "        #printM3(all_z,\"All Z\")\n",
    "        #print(f\"\\nIdeals:{ideals}\")\n",
    "        result = []\n",
    "        next = []\n",
    "        for i in range(len(self.layers)):\n",
    "            if i == 0:\n",
    "                #print(f\"\\nBack\")\n",
    "                aux, next = self.layers[-(i+1)].backpropagation(all_a[-(i+1)], all_a[-(i+2)], all_z[-(i+1)], ideals)\n",
    "                result.append(aux)\n",
    "                #print(next)\n",
    "            else:\n",
    "                #print(f\"\\nBack Hidden {i}\")\n",
    "                aux = self.layers[-(i+1)].backpropagationHidden(next, all_a[-(i+1)], all_a[-(i+2)], self.layers[-i].neurons)\n",
    "                result.append(aux)\n",
    "        return result\n",
    "        \n",
    "    def calcError(self, output, ideals):\n",
    "        result = 0\n",
    "        #print(f\"Ideals:{ideals}\\nOut:{output}\")\n",
    "        for i in range(len(output)):\n",
    "            result = result + calcCost(output[i], ideals[i])\n",
    "        #print(f\"Total cost:{result}\")\n",
    "        return result\n",
    "    \n",
    "    def updateWeights(self, update_values, alpha):\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[-(i+1)].updateWeights(update_values[i],alpha)\n",
    "\n",
    "    def train(self, input, ideals, alpha, epoch):\n",
    "        print(\"Training\")\n",
    "        acumulated = 0\n",
    "        for i in range(epoch):\n",
    "            error = []\n",
    "            for j in range(len(input)):\n",
    "                all_a, all_z = self.foward(input[j])\n",
    "                #print(f\"\\nAll Activations:\\n{all_a}\")\n",
    "                if(i <= 2 or i >= 998):\n",
    "                    error.append(self.calcError(all_a[-1],ideals[j]))\n",
    "                fix = self.backpropagation(all_a, all_z, ideals[j])\n",
    "                #print(f\"fix:{fix}\")\n",
    "                if type(acumulated) is int:\n",
    "                    acumulated = fix\n",
    "                else:\n",
    "                    for k in range(len(acumulated)):\n",
    "                        for l in range(len(acumulated[k])):\n",
    "                            acumulated[k][l] = acumulated[k][l] + fix[k][l]\n",
    "                #print(f\"\\nacc {j}:\\n{acumulated}\")\n",
    "            if(i <= 2 or i >= epoch-2):\n",
    "                error = sum(error)\n",
    "                error = error / len(input)\n",
    "                print(f\"Total Error Epoch {i}: {error}\")\n",
    "            #print(f\"Acumulated sum:{acumulated}\")\n",
    "            for k in range(len(acumulated)):\n",
    "                for l in range(len(acumulated[k])):\n",
    "                    acumulated[k][l] = acumulated[k][l] / len(input)\n",
    "            #print(f\"Acumulated div:{acumulated}\")\n",
    "            self.updateWeights(acumulated,alpha)\n",
    "\n",
    "    def test(self, input, ideals):\n",
    "        print(\"Testing\")\n",
    "        error = []\n",
    "        for j in range(len(input)):\n",
    "            all_a, all_z = self.foward(input[j])\n",
    "            error.append(self.calcError(all_a[-1],ideals[j]))\n",
    "        error = sum(error)\n",
    "        error = error / len(input)\n",
    "        print(f\"Total Error:{error}\")\n",
    "    \n",
    "    def relat(self):\n",
    "        for i in range(len(self.layers)):\n",
    "            print(f\"\\nLayer{i}:\")\n",
    "            self.layers[i].relat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "76244f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NEURONS:[[0.53505281 0.48067163 0.01322175 0.89639771 0.88677219]\n",
      " [0.03482259 0.92567307 0.53967346 0.69864528 0.69220854]\n",
      " [0.19529239 0.00302992 0.44629357 0.5240007  0.79206165]\n",
      " [0.95705507 0.88678711 0.36716265 0.43427406 0.69204985]\n",
      " [0.00763542 0.05667083 0.73754472 0.9619466  0.51904677]\n",
      " [0.45343916 0.06253496 0.62399692 0.94321288 0.39924876]\n",
      " [0.49617239 0.98929866 0.41295446 0.45439981 0.17472523]\n",
      " [0.09275761 0.65247683 0.01826401 0.15739375 0.82830725]]\n",
      "\n",
      "NEURONS:[[0.44537192 0.21518127 0.41157327 0.34226803 0.31596793 0.94241969\n",
      "  0.98593782 0.6898981  0.45942567]\n",
      " [0.7302051  0.65108849 0.11908495 0.13888099 0.62663989 0.52569359\n",
      "  0.71893059 0.63195558 0.14067143]\n",
      " [0.04703326 0.93823256 0.41383944 0.75651595 0.4575987  0.91665408\n",
      "  0.36531643 0.48181913 0.54214596]\n",
      " [0.4750006  0.24654197 0.88258276 0.44431443 0.16435661 0.26430209\n",
      "  0.13308391 0.25551694 0.38146345]]\n",
      "\n",
      "NEURONS:[[0.52883315 0.64622763 0.08830572 0.17138982 0.9301475 ]]\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "network.appendLayer(8,4,sigmoid,dsigmoid)\n",
    "network.appendLayer(4,8,sigmoid,dsigmoid)\n",
    "network.appendLayer(1,4,sigmoid,dsigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b2766baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer0:\n",
      "[[0.53505281 0.48067163 0.01322175 0.89639771 0.88677219]\n",
      " [0.03482259 0.92567307 0.53967346 0.69864528 0.69220854]\n",
      " [0.19529239 0.00302992 0.44629357 0.5240007  0.79206165]\n",
      " [0.95705507 0.88678711 0.36716265 0.43427406 0.69204985]\n",
      " [0.00763542 0.05667083 0.73754472 0.9619466  0.51904677]\n",
      " [0.45343916 0.06253496 0.62399692 0.94321288 0.39924876]\n",
      " [0.49617239 0.98929866 0.41295446 0.45439981 0.17472523]\n",
      " [0.09275761 0.65247683 0.01826401 0.15739375 0.82830725]]\n",
      "\n",
      "Layer1:\n",
      "[[0.44537192 0.21518127 0.41157327 0.34226803 0.31596793 0.94241969\n",
      "  0.98593782 0.6898981  0.45942567]\n",
      " [0.7302051  0.65108849 0.11908495 0.13888099 0.62663989 0.52569359\n",
      "  0.71893059 0.63195558 0.14067143]\n",
      " [0.04703326 0.93823256 0.41383944 0.75651595 0.4575987  0.91665408\n",
      "  0.36531643 0.48181913 0.54214596]\n",
      " [0.4750006  0.24654197 0.88258276 0.44431443 0.16435661 0.26430209\n",
      "  0.13308391 0.25551694 0.38146345]]\n",
      "\n",
      "Layer2:\n",
      "[[0.52883315 0.64622763 0.08830572 0.17138982 0.9301475 ]]\n",
      "Training\n",
      "Total Error Epoch 0: 0.33711436562805575\n",
      "Total Error Epoch 1: 0.33699479577746\n",
      "Total Error Epoch 2: 0.3368809352588345\n",
      "Total Error Epoch 9998: 0.16688974067382475\n",
      "Total Error Epoch 9999: 0.16688971470370087\n",
      "Testing\n",
      "Total Error:0.1668896887394991\n",
      "\n",
      "Layer0:\n",
      "[[-0.5954018  -0.92760086  1.1941193   1.53852149  0.52125773]\n",
      " [-1.2297199  -0.88654634  2.28219837  1.61387309  0.24136903]\n",
      " [-0.8147578  -1.54014553  2.19751256  1.43925815  0.40113683]\n",
      " [ 0.95863874  0.88782783  0.36764033  0.43435156  0.69238088]\n",
      " [-0.91410453 -1.28165837  2.11334185  1.68069427  0.18206542]\n",
      " [-0.7723212  -1.58226382  2.18765413  1.77686933 -0.02157026]\n",
      " [ 0.46082819  0.96459344  0.40520235  0.45398491  0.16725577]\n",
      " [-1.01708816 -1.22061643  2.28837916  1.32310888  0.36790286]]\n",
      "\n",
      "Layer1:\n",
      "[[ 0.37038928  0.51830588  0.77347665 -0.94873092  0.67864627  1.31020626\n",
      "  -0.30148222  0.99433255 -0.83179607]\n",
      " [ 0.66108417  1.5246556   1.09617459 -1.45893288  1.60391868  1.5165276\n",
      "  -0.87307722  1.50763227 -1.45749304]\n",
      " [-0.02976889  0.90672431  0.40244806  1.1720974   0.44730692  0.90451625\n",
      "   0.77990635  0.45079318  0.95778426]\n",
      " [ 0.6422654   1.05774484  1.78772066 -0.95562751  1.07031174  1.17968506\n",
      "  -1.26169313  1.06886694 -1.01879093]]\n",
      "\n",
      "Layer2:\n",
      "[[ 2.20728167  5.98051697 -1.70006197  4.43799931 -2.66774759]]\n"
     ]
    }
   ],
   "source": [
    "def add_offset(X):\n",
    "    n_samples = X.shape[0]\n",
    "    n_features = X.shape[1]\n",
    "\n",
    "    X_with_offset = np.zeros((n_samples, n_features + 1))\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        X_with_offset[i, 0] = 1.0\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_features):\n",
    "            X_with_offset[i, j + 1] = X[i, j]\n",
    "\n",
    "    X = X_with_offset\n",
    "\n",
    "    return X\n",
    "\n",
    "# normaliza as colunas de características (0 a 1)\n",
    "def normalize_x(X):\n",
    "    X = (X - X.mean()) / X.std()\n",
    "    return X\n",
    "\n",
    "# function to prepare X (normalize + add a offset)\n",
    "def prepare_X(X):\n",
    "    return add_offset(normalize_x(X))\n",
    "\n",
    "# map every possible result in a class number\n",
    "def prepare_Y(y):\n",
    "    mapping = {\n",
    "        \"Iris-setosa\": 0,\n",
    "        \"Iris-versicolor\": 1,\n",
    "        \"Iris-virginica\": 2,\n",
    "    }\n",
    "\n",
    "    return y.map(mapping)\n",
    "\n",
    "# Definição do dataframe iris Flowers\n",
    "df = pd.read_csv('IRIS.csv')\n",
    "\n",
    "\n",
    "X = df.drop(columns=['species']).to_numpy()\n",
    "\n",
    "Y = prepare_Y(df['species']).to_numpy()\n",
    "\n",
    "# Divisão entre treino e teste\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clean_input = []\n",
    "for i in range(len(X)):\n",
    "    clean_input.append(np.array(X[i]))\n",
    "\n",
    "Y = Y.tolist()\n",
    "\n",
    "clean_all_ideals = []\n",
    "for i in range(len(Y)):\n",
    "    aux = [Y[i]]\n",
    "    clean_all_ideals.append(aux)\n",
    "\n",
    "#print(clean_all_ideals)\n",
    "\n",
    "network.relat()\n",
    "\n",
    "network.train(clean_input, clean_all_ideals, 0.5, 10000)\n",
    "\n",
    "network.test(clean_input,clean_all_ideals)\n",
    "\n",
    "network.relat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53db805",
   "metadata": {},
   "source": [
    "network2 = Network2()\n",
    "network2.appendLayer(2,2,sigmoid,dsigmoid,[[0.15, 0.2, 0.35], [0.25, 0.3, 0.35]])\n",
    "network2.appendLayer(2,2,sigmoid,dsigmoid,[[0.4, 0.45, 0.6], [0.5, 0.55, 0.6]])\n",
    "\n",
    "input = [[1,1],[0.05,0.1],[1,0],[0,0]]\n",
    "all_ideals = [[0,1],[0.01,0.99],[1,0],[0,1]]\n",
    "\n",
    "clean_input = []\n",
    "for i in range(len(input)):\n",
    "    clean_input.append(np.array(input[i]))\n",
    "\n",
    "clean_all_ideals = np.array(all_ideals)\n",
    "\n",
    "network2.train([clean_input[1]], [clean_all_ideals[1]], 0.5, 10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
