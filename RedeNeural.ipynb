{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "52bb258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3e191889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printM3(matrix3, text):\n",
    "    print(f\"\\n{text}:\\n[\")\n",
    "    for i in range(len(matrix3)):\n",
    "        print(matrix3[i])\n",
    "    print(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dbb0476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcFoward(values, weights):\n",
    "    #print(f\"values:{values}\\nweights:{weights}\")\n",
    "    result = 0.0\n",
    "    for i in range(len(weights)-1):\n",
    "        result += values[i] * weights[i]\n",
    "    result += 1 * weights[-1]\n",
    "    #print(f\"\\ncalcFoward:{result}\")\n",
    "    return result\n",
    "\n",
    "def calcCost(value, ideal):\n",
    "    result = (ideal - value)**2/2\n",
    "    #print(f\"Cost:{result}\")\n",
    "    return result\n",
    "\n",
    "def calcdETotalGo(value, ideal):\n",
    "    result = -(ideal - value)\n",
    "    #print(f\"dETotal:{result}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "452b2eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(value):\n",
    "    if(value < 0):\n",
    "        value = 0\n",
    "    print(f\"\\relu:{value}\")\n",
    "    return value\n",
    "\n",
    "def drelu(value):\n",
    "    if(value > 0):\n",
    "        return value\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def linear(value):\n",
    "    return value\n",
    "\n",
    "def dlinear(value):\n",
    "    return 1\n",
    "\n",
    "def sigmoid(value):\n",
    "    try:\n",
    "        value = (1/(1 + np.exp(-value)))\n",
    "    except RuntimeWarning:\n",
    "        value = np.finfo(np.float64).tiny\n",
    "    return value\n",
    "\n",
    "def dsigmoid(value):\n",
    "    value = value * (1 - value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ef783a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self,layer_size, neuron_size, activation_function, dactivation_function):\n",
    "        self.neurons = np.random.rand(layer_size, neuron_size+1)\n",
    "        #matrix = np.array(weights)\n",
    "        #self.neurons = matrix\n",
    "        print(f\"\\nNEURONS:{self.neurons}\")\n",
    "        self.activation_function = activation_function\n",
    "        self.dactivation_function = dactivation_function\n",
    "    \n",
    "    def updateWeights(self, update_values, alpha):\n",
    "        #print(f\"Udate Weights\\nUpdate Values:{update_values}\\nAlpha:{alpha}\")\n",
    "        for i in range(len(self.neurons)):\n",
    "            #print(f\"\\nNeuron {i}\")\n",
    "            for j in range(len(self.neurons[i])):\n",
    "                #print(f\"\\nWeight {j}\")\n",
    "                #print(f\"\\nValue:{update_values[i][j]}\")\n",
    "                #print(f\"Equation\\n{self.neurons[i][j]} - {alpha} * {update_values[i][j]}\")\n",
    "                self.neurons[i][j] = self.neurons[i][j] - alpha * update_values[i][j]\n",
    "\n",
    "\n",
    "    def foward(self,values):\n",
    "        layer_a = []\n",
    "        layer_z = []\n",
    "        #print(f\"\\nValues:{values}\")\n",
    "        for i in range(len(self.neurons)):\n",
    "            #print(f\"\\nNeuron:{i}\")\n",
    "            z = calcFoward(values,self.neurons[i])\n",
    "            a = self.activation_function(z)\n",
    "            #print(f\"a:{a}\\nz:{z}\")\n",
    "            layer_a.append(a)\n",
    "            layer_z.append(z)\n",
    "            \n",
    "        #print(f\"\\nLayer A:{layer_a}\\nLayer Z:{layer_z}\")\n",
    "        return np.array(layer_a),np.array(layer_z)\n",
    "    \n",
    "    def backpropagation(self, a0, a1, z0, ideals):\n",
    "        #print(f\"A0:{a0}\\nA1:{a1}\\nZ:{z0}\\nIdeals:{ideals}\")\n",
    "        result = []\n",
    "        aux = []\n",
    "        for i in range(len(self.neurons)):\n",
    "            neuron = []\n",
    "            #print(f\"\\nNeuron:{i}\")\n",
    "            dEg = calcdETotalGo(a0[i],ideals[i])\n",
    "            #print(f\"dEg:{dEg}\")\n",
    "            dgouo = self.dactivation_function(a0[i])\n",
    "            #print(f\"dgouo:{dgouo}\")\n",
    "            #pesos\n",
    "            for j in range(len(self.neurons[i])-1):\n",
    "                #print(f\"\\nWeight:{j}\")\n",
    "                dgh = a1[j]\n",
    "                #print(f\"dgh:{dgh}\")\n",
    "                dEw = dEg * dgouo * dgh\n",
    "                #print(f\"dEw:{dEw}\")\n",
    "                neuron.append(dEw)\n",
    "            #bias\n",
    "            #print(\"\\nBias:\")\n",
    "            dEw = dEg * dgouo\n",
    "            #print(f\"dEw:{dEw}\")\n",
    "            neuron.append(dEw)\n",
    "            #print(f\"Fix Result:{neuron}\")\n",
    "            result.append(np.array(neuron))\n",
    "            aux.append(dEw)\n",
    "            #print(f\"aux {i}:\\n{aux}\")\n",
    "        return np.array(result), np.array(aux)\n",
    "\n",
    "    def backpropagationHidden(self, dEu, a0, a1, previous_layer):\n",
    "        #print(f\"\\nA0:{a0}\\nA1:{a1}\\ndEu:{dEu}\\nWeights:{self.neurons}\\nPrevious Layer:{previous_layer}\")\n",
    "        result = []\n",
    "        for i in range(len(self.neurons)):\n",
    "            neuron = []\n",
    "            #print(f\"\\nNeuron:{i}\")\n",
    "            dghuh = self.dactivation_function(a0[i])\n",
    "            #print(f\"dghuh:{dghuh}\")\n",
    "            true_dEg = []\n",
    "            for j in range(len(previous_layer)):\n",
    "                for k in range(len(dEu)):\n",
    "                    #print(f\"\\nCalc dEg:{j}\")\n",
    "                    dEg = previous_layer[j][i] * dEu[k] \n",
    "                    #print(f\"W:{previous_layer[j][i]}\\ndEu:{dEu[j]}\\ndEg:{dEg}\")\n",
    "                    true_dEg.append(dEg)\n",
    "            true_dEg = sum(true_dEg)\n",
    "            #print(f\"\\nTrue dEg:{true_dEg}\")\n",
    "            #pesos\n",
    "            for k in range(len(self.neurons[i])-1):\n",
    "                #print(f\"\\nWeight:{k}\")\n",
    "                dgh = a1[k]\n",
    "                #print(f\"dgh:{dgh}\")\n",
    "                dEw = true_dEg * dghuh * dgh\n",
    "                #print(f\"\\ndEw:{dEw}\")\n",
    "                neuron.append(dEw)\n",
    "            #bias\n",
    "            #print(f\"\\nBias:\")\n",
    "            dEw = true_dEg * dghuh\n",
    "            #print(f\"\\ndEw:{dEw}\")\n",
    "            neuron.append(dEw)\n",
    "            result.append(np.array(neuron))\n",
    "\n",
    "\n",
    "        return np.array(result)\n",
    "\n",
    "\n",
    "    def relat(self):\n",
    "        print(self.neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e9dd6f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def appendLayer(self, layer_size, neuron_size, activation_function, dactivation_function):\n",
    "        self.layers.append(Layer(layer_size, neuron_size, activation_function, dactivation_function))\n",
    "\n",
    "    def foward(self,values):\n",
    "        #print(f\"\\nFirst_Values:{values}\")\n",
    "        layer_a = [values]\n",
    "        layer_z = []\n",
    "        for i in range(len(self.layers)):\n",
    "            #print(f\"\\nFoward:{i}\")\n",
    "            values,z = self.layers[i].foward(values)\n",
    "            layer_a.append(values)\n",
    "            layer_z.append(z)\n",
    "        return layer_a,layer_z\n",
    "                \n",
    "    def backpropagation(self, all_a, all_z, ideals):\n",
    "        #print(f\"\\nBack First\")\n",
    "        #printM3(all_a,\"All A\")\n",
    "        #printM3(all_z,\"All Z\")\n",
    "        #print(f\"\\nIdeals:{ideals}\")\n",
    "        result = []\n",
    "        next = []\n",
    "        for i in range(len(self.layers)):\n",
    "            if i == 0:\n",
    "                #print(f\"\\nBack\")\n",
    "                aux, next = self.layers[-(i+1)].backpropagation(all_a[-(i+1)], all_a[-(i+2)], all_z[-(i+1)], ideals)\n",
    "                result.append(aux)\n",
    "                #print(next)\n",
    "            else:\n",
    "                #print(f\"\\nBack Hidden {i}\")\n",
    "                aux = self.layers[-(i+1)].backpropagationHidden(next, all_a[-(i+1)], all_a[-(i+2)], self.layers[-i].neurons)\n",
    "                result.append(aux)\n",
    "        return result\n",
    "        \n",
    "    def calcError(self, output, ideals):\n",
    "        result = 0\n",
    "        #print(f\"Ideals:{ideals}\\nOut:{output}\")\n",
    "        for i in range(len(output)):\n",
    "            result = result + calcCost(output[i], ideals[i])\n",
    "        #print(f\"Total cost:{result}\")\n",
    "        return result\n",
    "    \n",
    "    def updateWeights(self, update_values, alpha):\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[-(i+1)].updateWeights(update_values[i],alpha)\n",
    "\n",
    "    def train(self, input, ideals, alpha, epoch):\n",
    "        print(\"Training\")\n",
    "        acumulated = 0\n",
    "        for i in range(epoch):\n",
    "            error = []\n",
    "            for j in range(len(input)):\n",
    "                all_a, all_z = self.foward(input[j])\n",
    "                #print(f\"\\nAll Activations:\\n{all_a}\")\n",
    "                if(i <= 2 or i >= 998):\n",
    "                    error.append(self.calcError(all_a[-1],ideals[j]))\n",
    "                fix = self.backpropagation(all_a, all_z, ideals[j])\n",
    "                #print(f\"fix:{fix}\")\n",
    "                if type(acumulated) is int:\n",
    "                    acumulated = fix\n",
    "                else:\n",
    "                    for k in range(len(acumulated)):\n",
    "                        for l in range(len(acumulated[k])):\n",
    "                            acumulated[k][l] = acumulated[k][l] + fix[k][l]\n",
    "                #print(f\"\\nacc {j}:\\n{acumulated}\")\n",
    "            if(i <= 2 or i >= epoch-2):\n",
    "                error = sum(error)\n",
    "                error = error / len(input)\n",
    "                print(f\"Total Error Epoch {i}: {error}\")\n",
    "            #print(f\"Acumulated sum:{acumulated}\")\n",
    "            for k in range(len(acumulated)):\n",
    "                for l in range(len(acumulated[k])):\n",
    "                    acumulated[k][l] = acumulated[k][l] / len(input)\n",
    "            #print(f\"Acumulated div:{acumulated}\")\n",
    "            self.updateWeights(acumulated,alpha)\n",
    "\n",
    "    def test(self, input, ideals):\n",
    "        print(\"Testing\")\n",
    "        error = []\n",
    "        for j in range(len(input)):\n",
    "            all_a, all_z = self.foward(input[j])\n",
    "            error.append(self.calcError(all_a[-1],ideals[j]))\n",
    "        error = sum(error)\n",
    "        error = error / len(input)\n",
    "        print(f\"Total Error:{error}\")\n",
    "    \n",
    "    def relat(self):\n",
    "        for i in range(len(self.layers)):\n",
    "            print(f\"\\nLayer{i}:\")\n",
    "            self.layers[i].relat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "76244f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NEURONS:[[3.68037054e-01 8.99558486e-02 6.09845415e-01 5.24026639e-01\n",
      "  7.13773111e-01]\n",
      " [2.93765499e-01 9.17158132e-01 3.22362118e-01 9.66606927e-01\n",
      "  5.55570566e-01]\n",
      " [2.05686213e-01 7.63880979e-01 3.86681183e-01 3.17231645e-02\n",
      "  1.61079958e-01]\n",
      " [1.14817897e-01 4.53867084e-01 4.28112025e-01 6.49160044e-01\n",
      "  9.75435580e-01]\n",
      " [7.86913423e-01 8.81816952e-01 4.28965946e-01 2.89487113e-01\n",
      "  7.43037341e-01]\n",
      " [3.71417978e-01 4.76145857e-01 5.19167356e-04 1.44269252e-01\n",
      "  4.32358613e-01]\n",
      " [4.65023209e-01 1.23719956e-01 2.98985607e-01 7.37773394e-01\n",
      "  9.61105286e-01]\n",
      " [7.62989423e-01 6.30842233e-01 4.09526752e-01 9.58867002e-01\n",
      "  3.04793236e-01]]\n",
      "\n",
      "NEURONS:[[0.29578293 0.16456661 0.65555235 0.17680255 0.4462669  0.61042368\n",
      "  0.71575867 0.74567201 0.72024706]\n",
      " [0.95030192 0.18896253 0.33356831 0.53998022 0.17459811 0.82707011\n",
      "  0.19095612 0.17147505 0.46701326]\n",
      " [0.38057975 0.54111403 0.80528694 0.19595217 0.45807228 0.64730089\n",
      "  0.11385307 0.34457362 0.56690039]\n",
      " [0.77993969 0.90923817 0.82435386 0.90712171 0.31082272 0.02053213\n",
      "  0.10190133 0.68752325 0.51605509]]\n",
      "\n",
      "NEURONS:[[0.54671349 0.49456172 0.20229292 0.29569571 0.91629778]]\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "network.appendLayer(8,4,sigmoid,dsigmoid)\n",
    "network.appendLayer(4,8,sigmoid,dsigmoid)\n",
    "network.appendLayer(1,4,sigmoid,dsigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b2766baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer0:\n",
      "[[3.68037054e-01 8.99558486e-02 6.09845415e-01 5.24026639e-01\n",
      "  7.13773111e-01]\n",
      " [2.93765499e-01 9.17158132e-01 3.22362118e-01 9.66606927e-01\n",
      "  5.55570566e-01]\n",
      " [2.05686213e-01 7.63880979e-01 3.86681183e-01 3.17231645e-02\n",
      "  1.61079958e-01]\n",
      " [1.14817897e-01 4.53867084e-01 4.28112025e-01 6.49160044e-01\n",
      "  9.75435580e-01]\n",
      " [7.86913423e-01 8.81816952e-01 4.28965946e-01 2.89487113e-01\n",
      "  7.43037341e-01]\n",
      " [3.71417978e-01 4.76145857e-01 5.19167356e-04 1.44269252e-01\n",
      "  4.32358613e-01]\n",
      " [4.65023209e-01 1.23719956e-01 2.98985607e-01 7.37773394e-01\n",
      "  9.61105286e-01]\n",
      " [7.62989423e-01 6.30842233e-01 4.09526752e-01 9.58867002e-01\n",
      "  3.04793236e-01]]\n",
      "\n",
      "Layer1:\n",
      "[[0.29578293 0.16456661 0.65555235 0.17680255 0.4462669  0.61042368\n",
      "  0.71575867 0.74567201 0.72024706]\n",
      " [0.95030192 0.18896253 0.33356831 0.53998022 0.17459811 0.82707011\n",
      "  0.19095612 0.17147505 0.46701326]\n",
      " [0.38057975 0.54111403 0.80528694 0.19595217 0.45807228 0.64730089\n",
      "  0.11385307 0.34457362 0.56690039]\n",
      " [0.77993969 0.90923817 0.82435386 0.90712171 0.31082272 0.02053213\n",
      "  0.10190133 0.68752325 0.51605509]]\n",
      "\n",
      "Layer2:\n",
      "[[0.54671349 0.49456172 0.20229292 0.29569571 0.91629778]]\n",
      "Training\n",
      "Total Error Epoch 0: 0.33656501902240055\n",
      "Total Error Epoch 1: 0.3365632700011518\n",
      "Total Error Epoch 2: 0.33656151112081034\n",
      "Total Error Epoch 998: 0.3354072898671671\n",
      "Total Error Epoch 999: 0.335406527502649\n",
      "Testing\n",
      "Total Error:0.335405765661181\n",
      "\n",
      "Layer0:\n",
      "[[0.30891577 0.04910733 0.59410327 0.52193207 0.70170736]\n",
      " [0.28847501 0.91350337 0.32110567 0.96650184 0.55446081]\n",
      " [0.18142696 0.74458555 0.38694176 0.03404324 0.15552639]\n",
      " [0.07627476 0.42668582 0.41954567 0.64860257 0.96741922]\n",
      " [0.78657503 0.8815836  0.42888604 0.28948068 0.74296569]\n",
      " [0.36578505 0.46428586 0.01844975 0.15320631 0.42956343]\n",
      " [0.45029159 0.11341422 0.29538523 0.73741746 0.95806119]\n",
      " [0.76121133 0.62963578 0.40903781 0.95880178 0.30442352]]\n",
      "\n",
      "Layer1:\n",
      "[[0.29606402 0.16481896 0.65581467 0.1770797  0.44651566 0.61068147\n",
      "  0.71602453 0.74592164 0.72049548]\n",
      " [0.95076793 0.18937685 0.33400068 0.54043918 0.17500597 0.82749462\n",
      "  0.19139473 0.17188448 0.46742053]\n",
      " [0.38075961 0.54127567 0.80545487 0.19612949 0.45823165 0.64746594\n",
      "  0.11402324 0.34473354 0.56705956]\n",
      " [0.78002511 0.90931378 0.8244329  0.90720579 0.31089711 0.02060971\n",
      "  0.10198155 0.68759794 0.51612937]]\n",
      "\n",
      "Layer2:\n",
      "[[0.59453668 0.54202567 0.24984019 0.34372453 0.96456044]]\n"
     ]
    }
   ],
   "source": [
    "def add_offset(X):\n",
    "    n_samples = X.shape[0]\n",
    "    n_features = X.shape[1]\n",
    "\n",
    "    X_with_offset = np.zeros((n_samples, n_features + 1))\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        X_with_offset[i, 0] = 1.0\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_features):\n",
    "            X_with_offset[i, j + 1] = X[i, j]\n",
    "\n",
    "    X = X_with_offset\n",
    "\n",
    "    return X\n",
    "\n",
    "# normaliza as colunas de características (0 a 1)\n",
    "def normalize_x(X):\n",
    "    X = (X - X.mean()) / X.std()\n",
    "    return X\n",
    "\n",
    "# function to prepare X (normalize + add a offset)\n",
    "def prepare_X(X):\n",
    "    return add_offset(normalize_x(X))\n",
    "\n",
    "# map every possible result in a class number\n",
    "def prepare_Y(y):\n",
    "    mapping = {\n",
    "        \"Iris-setosa\": 0,\n",
    "        \"Iris-versicolor\": 1,\n",
    "        \"Iris-virginica\": 2,\n",
    "    }\n",
    "\n",
    "    return y.map(mapping)\n",
    "\n",
    "# Definição do dataframe iris Flowers\n",
    "df = pd.read_csv('IRIS.csv')\n",
    "\n",
    "\n",
    "X = df.drop(columns=['species']).to_numpy()\n",
    "\n",
    "Y = prepare_Y(df['species']).to_numpy()\n",
    "\n",
    "# Divisão entre treino e teste\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clean_input = []\n",
    "for i in range(len(X)):\n",
    "    clean_input.append(np.array(X[i]))\n",
    "\n",
    "Y = Y.tolist()\n",
    "\n",
    "clean_all_ideals = []\n",
    "for i in range(len(Y)):\n",
    "    aux = [Y[i]]\n",
    "    clean_all_ideals.append(aux)\n",
    "\n",
    "#print(clean_all_ideals)\n",
    "\n",
    "network.relat()\n",
    "\n",
    "network.train(clean_input, clean_all_ideals, 0.01, 1000)\n",
    "\n",
    "network.test(clean_input,clean_all_ideals)\n",
    "\n",
    "network.relat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53db805",
   "metadata": {},
   "source": [
    "network2 = Network2()\n",
    "network2.appendLayer(2,2,sigmoid,dsigmoid,[[0.15, 0.2, 0.35], [0.25, 0.3, 0.35]])\n",
    "network2.appendLayer(2,2,sigmoid,dsigmoid,[[0.4, 0.45, 0.6], [0.5, 0.55, 0.6]])\n",
    "\n",
    "input = [[1,1],[0.05,0.1],[1,0],[0,0]]\n",
    "all_ideals = [[0,1],[0.01,0.99],[1,0],[0,1]]\n",
    "\n",
    "clean_input = []\n",
    "for i in range(len(input)):\n",
    "    clean_input.append(np.array(input[i]))\n",
    "\n",
    "clean_all_ideals = np.array(all_ideals)\n",
    "\n",
    "network2.train([clean_input[1]], [clean_all_ideals[1]], 0.5, 10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
