{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "52bb258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "3e191889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printM3(matrix3, text):\n",
    "    print(f\"\\n{text}:\\n[\")\n",
    "    for i in range(len(matrix3)):\n",
    "        print(matrix3[i])\n",
    "    print(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "dbb0476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcFoward(values, weights):\n",
    "    #print(f\"values:{values}\\nweights:{weights}\")\n",
    "    result = 0.0\n",
    "    for i in range(len(weights)-1):\n",
    "        result += values[i] * weights[i]\n",
    "    result += 1 * weights[-1]\n",
    "    #print(f\"\\ncalcFoward:{result}\")\n",
    "    return result\n",
    "\n",
    "def calcCost(value, ideal):\n",
    "    result = (ideal - value)**2/2\n",
    "    #print(f\"Cost:{result}\")\n",
    "    return result\n",
    "\n",
    "def calcdETotalGo(value, ideal):\n",
    "    result = -(ideal - value)\n",
    "    #print(f\"dETotal:{result}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "452b2eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(value):\n",
    "    if(value < 0):\n",
    "        value = 0\n",
    "    print(f\"\\relu:{value}\")\n",
    "    return value\n",
    "\n",
    "def drelu(value):\n",
    "    if(value > 0):\n",
    "        return value\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def linear(value):\n",
    "    return value\n",
    "\n",
    "def dlinear(value):\n",
    "    return 1\n",
    "\n",
    "def sigmoid(value):\n",
    "    try:\n",
    "        value = (1/(1 + np.exp(-value)))\n",
    "    except OverflowError:\n",
    "        value = np.finfo(np.float64).tiny\n",
    "    return value\n",
    "\n",
    "def dsigmoid(value):\n",
    "    value = value * (1 - value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "ef783a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self,layer_size, neuron_size, activation_function, dactivation_function):\n",
    "        self.neurons = np.random.rand(layer_size, neuron_size+1)\n",
    "        #matrix = np.array(weights)\n",
    "        #self.neurons = matrix\n",
    "        print(f\"\\nNEURONS:{self.neurons}\")\n",
    "        self.activation_function = activation_function\n",
    "        self.dactivation_function = dactivation_function\n",
    "    \n",
    "    def updateWeights(self, update_values, alpha):\n",
    "        #print(f\"Udate Weights\\nUpdate Values:{update_values}\\nAlpha:{alpha}\")\n",
    "        for i in range(len(self.neurons)):\n",
    "            #print(f\"\\nNeuron {i}\")\n",
    "            for j in range(len(self.neurons[i])):\n",
    "                #print(f\"\\nWeight {j}\")\n",
    "                #print(f\"\\nValue:{update_values[i][j]}\")\n",
    "                #print(f\"Equation\\n{self.neurons[i][j]} - {alpha} * {update_values[i][j]}\")\n",
    "                self.neurons[i][j] = self.neurons[i][j] - alpha * update_values[i][j]\n",
    "\n",
    "\n",
    "    def foward(self,values):\n",
    "        layer_a = []\n",
    "        layer_z = []\n",
    "        for i in range(len(self.neurons)):\n",
    "            #print(f\"\\nNeuron:{i}\")\n",
    "            z = calcFoward(values,self.neurons[i])\n",
    "            a = self.activation_function(z)\n",
    "            #print(f\"a:{a}\\nz:{z}\")\n",
    "            layer_a.append(a)\n",
    "            layer_z.append(z)\n",
    "        #print(f\"\\nLayer A:{layer_a}\\nLayer Z:{layer_z}\")\n",
    "        return np.array(layer_a),np.array(layer_z)\n",
    "    \n",
    "    def backpropagation(self, a0, a1, z0, ideals):\n",
    "        #print(f\"A0:{a0}\\nA1:{a1}\\nZ:{z0}\\nIdeals:{ideals}\")\n",
    "        result = []\n",
    "        aux = []\n",
    "        for i in range(len(self.neurons)):\n",
    "            neuron = []\n",
    "            #print(f\"\\nNeuron:{i}\")\n",
    "            dEg = calcdETotalGo(a0[i],ideals[i])\n",
    "            #print(f\"dEg:{dEg}\")\n",
    "            dgouo = self.dactivation_function(a0[i])\n",
    "            #print(f\"dgouo:{dgouo}\")\n",
    "            #pesos\n",
    "            for j in range(len(self.neurons[i])-1):\n",
    "                #print(f\"\\nWeight:{j}\")\n",
    "                dgh = a1[j]\n",
    "                #print(f\"dgh:{dgh}\")\n",
    "                dEw = dEg * dgouo * dgh\n",
    "                #print(f\"dEw:{dEw}\")\n",
    "                neuron.append(dEw)\n",
    "            #bias\n",
    "            #print(\"\\nBias:\")\n",
    "            dEw = dEg * dgouo\n",
    "            #print(f\"dEw:{dEw}\")\n",
    "            neuron.append(dEw)\n",
    "            #print(f\"Neuron Result:{neuron}\")\n",
    "            result.append(np.array(neuron))\n",
    "            aux.append(dEw)\n",
    "        return np.array(result), np.array(aux)\n",
    "\n",
    "    def backpropagationHidden(self, bias, a0, a1, next_layer_size):\n",
    "        #print(f\"\\nA0:{a0}\\nA1:{a1}\\nBias2:{bias}\\nWeights:{self.neurons}\\nNext Layer Size:{next_layer_size}\")\n",
    "        result = []\n",
    "        for i in range(len(self.neurons)):\n",
    "            neuron = []\n",
    "            #print(f\"\\nNeuron:{i}\")\n",
    "            dghuh = self.dactivation_function(a0[i])\n",
    "            #print(f\"dghuh:{dghuh}\")\n",
    "            true_dEg = []\n",
    "            for j in range(next_layer_size):\n",
    "                #print(f\"\\nCalc dEw:{j}\")\n",
    "                dEg = self.neurons[j][i] * bias[j] \n",
    "                #print(f\"dEg:{dEg}\")\n",
    "                #print(f\"W:{self.neurons[j][i]}\\nB:{bias[j]}\\ndEg:{dEg}\")\n",
    "                true_dEg.append(dEg)\n",
    "            true_dEg = sum(true_dEg)\n",
    "            #print(f\"\\nTrue dEg:{true_dEg}\")\n",
    "            #pesos\n",
    "            for k in range(len(self.neurons[i])-1):\n",
    "                #print(f\"\\nWeight:{k}\")\n",
    "                dgh = a1[k]\n",
    "                #print(f\"dgh:{dgh}\")\n",
    "                dEw = true_dEg * dghuh * dgh\n",
    "                #print(f\"\\ndEw:{dEw}\")\n",
    "                neuron.append(dEw)\n",
    "            #bias\n",
    "            true_dEg = []\n",
    "            for j in range(next_layer_size):\n",
    "                #print(f\"\\nCalc dEw:{j}\")\n",
    "                dEg = self.neurons[j][i] * bias[j] \n",
    "                #print(f\"\\nB:{bias[j]}\\ndEg:{dEg}\")\n",
    "                true_dEg.append(dEg)\n",
    "            true_dEg = sum(true_dEg)\n",
    "            #print(f\"\\nTrue dEg:{true_dEg}\")\n",
    "            dEw = true_dEg * dghuh\n",
    "            #print(f\"\\ndEw:{dEw}\")\n",
    "            neuron.append(dEw)\n",
    "            result.append(np.array(neuron))\n",
    "\n",
    "\n",
    "        return np.array(result)\n",
    "\n",
    "\n",
    "    def relat(self):\n",
    "        print(self.neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "e9dd6f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def appendLayer(self, layer_size, neuron_size, activation_function, dactivation_function):\n",
    "        self.layers.append(Layer(layer_size, neuron_size, activation_function, dactivation_function))\n",
    "\n",
    "    def foward(self,values):\n",
    "        #print(f\"\\nFirst_Values:{values}\")\n",
    "        layer_a = [values]\n",
    "        layer_z = []\n",
    "        for i in range(len(self.layers)):\n",
    "            #print(f\"\\nFoward:{i}\")\n",
    "            values,z = self.layers[i].foward(values)\n",
    "            layer_a.append(values)\n",
    "            layer_z.append(z)\n",
    "        layer_a = np.array(layer_a)\n",
    "        layer_z = np.array(layer_z)\n",
    "        return layer_a,layer_z\n",
    "                \n",
    "    def backpropagation(self, all_a, all_z, ideals):\n",
    "        #print(f\"\\nBack First\")\n",
    "        #printM3(all_a,\"All A\")\n",
    "        #printM3(all_z,\"All Z\")\n",
    "        #print(f\"\\nIdeals:{ideals}\")\n",
    "        result = []\n",
    "        next = []\n",
    "        for i in range(len(self.layers)):\n",
    "            if i == 0:\n",
    "                #print(f\"\\nBack\")\n",
    "                aux, next = self.layers[i].backpropagation(all_a[-(i+1)], all_a[-(i+2)], all_z[-(i+1)], ideals)\n",
    "                result.append(aux)\n",
    "                #print(next)\n",
    "            else:\n",
    "                #print(f\"back hidden{i}\")\n",
    "                aux = self.layers[i].backpropagationHidden(next, all_a[-(i+1)], all_a[-(i+2)], len(self.layers[i-1].neurons))\n",
    "                result.append(aux)\n",
    "        return result\n",
    "        \n",
    "    def calcError(self, output, ideals):\n",
    "        result = 0\n",
    "        #print(f\"Ideals:{ideals}\\nOut:{output}\")\n",
    "        for i in range(len(output)):\n",
    "            result = result + calcCost(output[i], ideals[i])\n",
    "        #print(f\"Total cost:{result}\")\n",
    "        return result\n",
    "    \n",
    "    def updateWeights(self, update_values, alpha):\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[-(i+1)].updateWeights(update_values[i],alpha)\n",
    "\n",
    "    def train(self, input, ideal, alpha, epoch):\n",
    "        acumulated = 0\n",
    "        for i in range(epoch):\n",
    "            error = []\n",
    "            for j in range(len(input)):\n",
    "                all_a, all_z = self.foward(input[j])\n",
    "                if(i <= 2 or i >= 998):\n",
    "                    error.append(self.calcError(all_a[-1],ideal[j]))\n",
    "                fix = self.backpropagation(all_a, all_z, ideal[j])\n",
    "                #print(f\"fix:{fix}\")\n",
    "                if type(acumulated) is int:\n",
    "                    acumulated = np.array(fix)\n",
    "                else:\n",
    "                    acumulated = acumulated + np.array(fix)\n",
    "            if(i <= 2 or i >= 998):\n",
    "                error = sum(error)\n",
    "                error = error / len(input)\n",
    "                print(f\"Total Error:{error}\")\n",
    "            #print(f\"Acumulated sum:{acumulated}\")\n",
    "            acumulated = acumulated / len(input)\n",
    "            #print(f\"Acumulated div:{acumulated}\")\n",
    "            self.updateWeights(acumulated,alpha)\n",
    "\n",
    "    \n",
    "    def relat(self):\n",
    "        for i in range(len(self.layers)):\n",
    "            print(f\"\\nLayer{i}:\")\n",
    "            self.layers[i].relat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "b2766baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NEURONS:[[0.73553248 0.01516598 0.20415891]\n",
      " [0.68689205 0.03638947 0.00837353]]\n",
      "\n",
      "NEURONS:[[0.67842253 0.83204438 0.74995412]\n",
      " [0.64737772 0.25671382 0.74241484]]\n",
      "Total Error:0.35961728413739347\n",
      "Total Error:0.35375467401455274\n",
      "Total Error:0.346046140338806\n",
      "Total Error:0.009419453221351355\n",
      "Total Error:0.009391494919328287\n",
      "Total Error:0.009363677589521203\n",
      "\n",
      "Layer0:\n",
      "[[-1.96842389  2.09335397  0.73648209]\n",
      " [ 3.71350373 -3.59386324 -1.75461837]]\n",
      "\n",
      "Layer1:\n",
      "[[-2.15718355  4.44707694 -1.58193651]\n",
      " [ 2.43476423 -4.29305614  1.37573231]]\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "network.appendLayer(2,2,sigmoid,dsigmoid)\n",
    "network.appendLayer(2,2,sigmoid,dsigmoid)\n",
    "\n",
    "#network.relat()\n",
    "\n",
    "# C1 C2 R\n",
    "# 1  1  0\n",
    "# 0  1  1\n",
    "# 1  0  1\n",
    "# 0  0  0\n",
    "\n",
    "input = [[1,1],[0.05,0.1],[1,0],[0,0]]\n",
    "all_ideals = [[0,1],[0.01,0.99],[1,0],[0,1]]\n",
    "\n",
    "#input = [[0.05,0.1]]\n",
    "#all_ideals = [[0.01,0.99]]\n",
    "\n",
    "clean_input = []\n",
    "for i in range(len(input)):\n",
    "    clean_input.append(np.array(input[i]))\n",
    "\n",
    "#print(clean_input)\n",
    "\n",
    "clean_all_ideals = np.array(all_ideals)\n",
    "\n",
    "\n",
    "#all_a, all_z = network.foward(clean_input[1])\n",
    "\n",
    "#printM3(all_a,\"All A\")\n",
    "#printM3(all_z,\"All Z\")\n",
    "\n",
    "#fix = network.backpropagation(all_a, all_z, clean_all_ideals[1])\n",
    "\n",
    "#print(f\"\\nFix:{fix}\")\n",
    "\n",
    "#network.updateWeights(fix,0.5)\n",
    "\n",
    "network.train(clean_input, clean_all_ideals, 0.5, 1001)\n",
    "\n",
    "network.relat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
